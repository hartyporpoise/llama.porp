# ──────────────────────────────────────────────────────────────────────────
# Porpulsion + Ollama — Docker Compose
#
# Quick start:
#   docker compose up
#
# Then open:
#   http://localhost:8080  — porpulsion UI
#
# To set a default model (pre-selected in the UI):
#   DEFAULT_MODEL=llama3.2 docker compose up
# ──────────────────────────────────────────────────────────────────────────

services:

  # ── Ollama: runs the actual model inference ──────────────────────────────
  ollama:
    image: alpine/ollama:latest
    container_name: ollama
    environment:
      # Hide GPUs from Ollama so it runs pure CPU inference.
      CUDA_VISIBLE_DEVICES: ""
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"          # expose Ollama API directly if needed
    restart: unless-stopped
    mem_limit: 2g
    memswap_limit: 2g
    cpus: "1.0"
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 5s
      timeout: 10s
      retries: 12
      start_period: 10s

  # ── Porpulsion: primary instance on port 8080 ───────────────────────────
  porpulsion:
    build:
      context: .
      dockerfile: Dockerfile
    image: porpulsion:latest
    container_name: porpulsion
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      OLLAMA_URL: "http://ollama:11434"
      # Go runtime: make GC container-aware to prevent OOM kills.
      GOMEMLIMIT: "900MiB"
    command:
      - "--ollama-url"
      - "http://ollama:11434"
      - "--model"
      - "${DEFAULT_MODEL:-}"
      - "--port"
      - "8080"
    ports:
      - "${PORT:-8080}:8080"
    restart: unless-stopped
    mem_limit: 1g
    memswap_limit: 1g
    cpus: "1.0"
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 10s

volumes:
  ollama_data:
    driver: local
